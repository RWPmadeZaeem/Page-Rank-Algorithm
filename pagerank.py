# -*- coding: utf-8 -*-
"""PageRank.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZmCEAaI9DiUwTxziCoYgR0aXoiaE6sv7
"""

from collections import defaultdict

def create_web_graph(links):

    web_graph = defaultdict(list)


    for source, target in links:
        web_graph[source].append(target)

        web_graph[target]

    return dict(web_graph)

# Example usage:
links = [('PageA', 'PageB'), ('PageA', 'PageC'), ('PageB', 'PageD'), ('PageC', 'PageD')]
graph = create_web_graph(links)
print(graph)

def calculate_pagerank(graph, iterations, d=0.85, tolerance=1e-6):

    N = len(graph)  # Total number of pages
    pagerank = {page: 1 / N for page in graph}  # Initialize PageRank values
    converged = False

    for _ in range(iterations):
        new_pagerank = {}
        delta = 0  # Track the sum of absolute differences between current and previous PageRank values

        for page in graph:

            incoming_pagerank = sum(pagerank[inlink] / len(graph[inlink]) for inlink in graph if page in graph[inlink])
            new_pagerank[page] = (1 - d) / N + d * incoming_pagerank
            delta += abs(new_pagerank[page] - pagerank[page])

        # Check for convergence
        if delta < tolerance:
            converged = True
            break

        pagerank = new_pagerank

    if converged:
        print("Algorithm converged after", _+1, "iterations.")
    else:
        print("Algorithm did not converge after", iterations, "iterations.")

    return pagerank

# Example usage:
graph = {'PageA': ['PageB', 'PageC'], 'PageB': ['PageA', 'PageC'], 'PageC': ['PageA', 'PageB']}
iterations = 100
pagerank = calculate_pagerank(graph, iterations)
print("PageRank values after convergence:")
print(pagerank)

import networkx as nx
import matplotlib.pyplot as plt

# Function to calculate PageRank
def calculate_pagerank(graph, iterations, d=0.85, tolerance=1e-6):
    # Implementation of calculate_pagerank function from the previous response goes here...
    N = len(graph)  # Total number of pages
    pagerank = {page: 1 / N for page in graph}  # Initialize PageRank values
    converged = False

    for _ in range(iterations):
        new_pagerank = {}
        delta = 0  # Track the sum of absolute differences between current and previous PageRank values

        for page in graph:

            incoming_pagerank = sum(pagerank[inlink] / len(graph[inlink]) for inlink in graph if page in graph[inlink])
            new_pagerank[page] = (1 - d) / N + d * incoming_pagerank
            delta += abs(new_pagerank[page] - pagerank[page])

        # Check for convergence
        if delta < tolerance:
            converged = True
            break

        pagerank = new_pagerank

    if converged:
        print("Algorithm converged after", _+1, "iterations.")
    else:
        print("Algorithm did not converge after", iterations, "iterations.")

    return pagerank


# Define the web graph
graph = {'PageA': ['PageB', 'PageC'], 'PageB': ['PageA', 'PageC'], 'PageC': ['PageA', 'PageB']}

# Calculate PageRank scores
pagerank = calculate_pagerank(graph, iterations=100)

# Create a NetworkX graph from the web graph dictionary
G = nx.Graph(graph)

# Set node sizes based on PageRank scores
node_sizes = [pagerank[node] * 10000 for node in G.nodes]

# Draw the graph with node sizes proportional to PageRank scores
plt.figure(figsize=(10, 8))
pos = nx.spring_layout(G)  # Set layout for better visualization
nx.draw(G, pos, with_labels=True, node_size=node_sizes, node_color='skyblue', font_size=12, font_weight='bold', edge_color='gray')
plt.title('Web Graph with Node Sizes Proportional to PageRank Scores')
plt.show()